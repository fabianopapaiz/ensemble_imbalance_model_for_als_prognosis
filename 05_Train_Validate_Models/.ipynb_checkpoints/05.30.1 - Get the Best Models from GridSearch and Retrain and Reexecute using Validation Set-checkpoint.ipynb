{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "#needed to import utils.py\n",
    "sys.path.append('../') \n",
    "\n",
    "import utils\n",
    "import utils_preprocessing\n",
    "import utils_exec_models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and save the 10 best performances obtained by GridSearch grouping by ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "SVM - SVC\n",
      "---------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "10 samples were saved\n",
      "--------------------------\n",
      "Naïve Bayes - ComplementNB\n",
      "--------------------------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "10 samples were saved\n",
      "-------------------------------\n",
      "Neural Networks - MLPClassifier\n",
      "-------------------------------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "10 samples were saved\n",
      "--------------------------------------\n",
      "Decision Tree - DecisionTreeClassifier\n",
      "--------------------------------------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "0 samples were saved\n",
      "-----------------\n",
      "k-NN - GaussianNB\n",
      "-----------------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "10 samples were saved\n",
      "-----------------------------------------\n",
      "Random Forest - RadiusNeighborsClassifier\n",
      "-----------------------------------------\n",
      "   > SINGLE-MODEL (10 best)\n",
      "10 samples were saved\n",
      "   > ENSEMBLE-IMBALANCE (10 best)\n",
      "0 samples were saved\n"
     ]
    }
   ],
   "source": [
    "#read SINGLE-MODEL results\n",
    "csv_file = 'exec_results/results_Single_Model.csv'\n",
    "df_single_model = utils.read_csv(csv_file)\n",
    "# display(df_single_model.head(3))\n",
    "\n",
    "#read ENSEMBLE-IMBALANCE results\n",
    "csv_file = 'exec_results/results_Ensemble_Imbalance.csv'\n",
    "df_ens_imb = utils.read_csv(csv_file)\n",
    "# display(df_ens_imb.head(3))\n",
    "\n",
    "\n",
    "# get each algorithm analyzed\n",
    "algorithms = list(df_single_model.Model.unique())\n",
    "model_classes = list(df_single_model.Classifier.unique())\n",
    "# print(model_classes)\n",
    "# print(algorithms)\n",
    "\n",
    "dir_dest = os.path.abspath('exec_results/')\n",
    "\n",
    "# store all best models Object for both scenarios\n",
    "best_models = list()\n",
    "\n",
    "n_to_save = 10\n",
    "\n",
    "for algorithm, model_class in zip(algorithms, model_classes):\n",
    "    utils.print_string_with_separators(f'{algorithm} - {model_class}')\n",
    "    \n",
    "    # ==============================================================     \n",
    "    # get the \"n\" best performances for the SINGLE-MODEL scenario\n",
    "    # ==============================================================     \n",
    "    print(f'   > SINGLE-MODEL ({n_to_save} best)')\n",
    "    df_best_single_model = df_single_model.loc[(\n",
    "          (df_single_model.Model == algorithm) \n",
    "    )].copy()\n",
    "\n",
    "    \n",
    "    # get and save the 5 best results for the algorithm    \n",
    "    df_best_single_model = df_best_single_model.head(n_to_save)\n",
    "#     display(df_best_single_model)\n",
    "    csv_file = f'{dir_dest}/best_performances_{algorithm}_in_SINGLE_MODEL.csv'\n",
    "    csv_file = csv_file.replace(' ', '_')\n",
    "    utils.save_to_csv(df=df_best_single_model, csv_file=csv_file)\n",
    "\n",
    "    # Get the best models and create instances using their hyperparameters    \n",
    "    for model in utils_exec_models.get_models_object_from_results(df_best_single_model):\n",
    "#         model['csv_file'] = csv_file\n",
    "        best_models.append(model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ==============================================================     \n",
    "    # get the \"n\" best performances for the ENSEMBLE-IMBALANCE scenario\n",
    "    # ==============================================================     \n",
    "    print(f'   > ENSEMBLE-IMBALANCE ({n_to_save} best)')\n",
    "    df_best_ens_imb = df_ens_imb.loc[(\n",
    "          (df_ens_imb.Estimator_Desc == algorithm) \n",
    "    )].copy()\n",
    "\n",
    "    \n",
    "    # get and save the 5 best results for the algorithm    \n",
    "    df_best_ens_imb = df_best_ens_imb.head(n_to_save)\n",
    "#     display(df_best_ens_imb)\n",
    "    csv_file = f'{dir_dest}/best_performances_{algorithm}_in_ENSEMBLE_IMBALANCE.csv'\n",
    "    csv_file = csv_file.replace(' ', '_')\n",
    "    utils.save_to_csv(df=df_best_ens_imb, csv_file=csv_file)\n",
    "    \n",
    "    # Get the best models and create instances using their hyperparameters    \n",
    "    for model in utils_exec_models.get_models_object_from_results(df_best_ens_imb):\n",
    "#         model['csv_file'] = csv_file\n",
    "        best_models.append(model)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Reexecute each best model as following:\n",
    "\n",
    " - ### Retrain the model using the full $Training$ set\n",
    " - ### Validate the model using the $Validation$ set \n",
    " - ### Save the $Validation$ $performances$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100: Executing SVM in \"Single-Model\" scenario\n",
      "2/100: Executing SVM in \"Single-Model\" scenario\n",
      "3/100: Executing SVM in \"Single-Model\" scenario\n",
      "4/100: Executing SVM in \"Single-Model\" scenario\n",
      "5/100: Executing SVM in \"Single-Model\" scenario\n",
      "6/100: Executing SVM in \"Single-Model\" scenario\n",
      "7/100: Executing SVM in \"Single-Model\" scenario\n",
      "8/100: Executing SVM in \"Single-Model\" scenario\n",
      "9/100: Executing SVM in \"Single-Model\" scenario\n",
      "10/100: Executing SVM in \"Single-Model\" scenario\n",
      "11/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "12/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "13/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "14/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "15/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "16/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "17/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "18/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "19/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "20/100: Executing Balanced-Bagging(SVM) in \"Ensemble-Imbalance\" scenario\n",
      "21/100: Executing NB in \"Single-Model\" scenario\n",
      "22/100: Executing NB in \"Single-Model\" scenario\n",
      "23/100: Executing NB in \"Single-Model\" scenario\n",
      "24/100: Executing NB in \"Single-Model\" scenario\n",
      "25/100: Executing NB in \"Single-Model\" scenario\n",
      "25 samples were saved\n",
      "CPU times: user 1min 9s, sys: 475 ms, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv_validation_performance = os.path.abspath('exec_results/validation_results.csv')\n",
    "\n",
    "\n",
    "\n",
    "# verify if already exists an CSV with the results\n",
    "overwrite_results_saved_previously = False\n",
    "\n",
    "if os.path.exists(csv_validation_performance) and overwrite_results_saved_previously==False:\n",
    "    print('Reading results saved previously...')\n",
    "    df_validation_performance = utils.read_csv(csv_file=csv_validation_performance)\n",
    "else:\n",
    "    df_validation_performance = None\n",
    "\n",
    "\n",
    "# Get the scaled Training and Validation subsets¶\n",
    "X_train, y_train, X_valid, y_valid = utils.get_train_and_validation_data(scaled=True)\n",
    "\n",
    "models_results = [] \n",
    "\n",
    "i = 1\n",
    "tot = len(best_models)\n",
    "\n",
    "\n",
    "\n",
    "## For each best model:\n",
    "#    1. Retrain using using the Training data\n",
    "#    2. Validate using the Validation data\n",
    "\n",
    "was_executed = False\n",
    "\n",
    "for model_info in best_models[:2]:\n",
    "\n",
    "    \n",
    "    try:    \n",
    "\n",
    "        # get the classifier name (without the parameters)\n",
    "        model_instance = model_info['model_instance']\n",
    "        scenario = model_info['Scenario']\n",
    "        model = model_info['Model']\n",
    "        model_params = model_info['Hyperparams']\n",
    "        estimator = model_info['Estimator']\n",
    "        estimator_params = model_info['Estimator_Hyperparams']\n",
    "\n",
    "        model_str = f'{model}' + ('' if scenario=='Single-Model' else f'({estimator})')\n",
    "\n",
    "        # check if model was already executed\n",
    "        if df_results is not None:\n",
    "            df_executed = df_results.loc[\n",
    "                (df_results.Estimator_Desc == str(estimator_desc))\n",
    "               &(df_results.Estimator_Class == str(estimator_name)) \n",
    "               &(df_results.Estimator_Hyperparams == str(estimator_params)) \n",
    "            ].copy()\n",
    "            \n",
    "            if df_executed.shape[0] > 0:\n",
    "                print(f'{i:>3} was already executed')\n",
    "                was_executed = True\n",
    "                continue\n",
    "\n",
    "        \n",
    "        print(f'{i}/{tot}: Executing {model_str} in \"{scenario}\" scenario',)\n",
    "        i += 1\n",
    "\n",
    "#         # Retrain using the full traning set\n",
    "#         model_instance.fit(X_train, y_train)\n",
    "\n",
    "#         #predict using the Validation set\n",
    "#         y_pred = model_instance.predict(X_valid)\n",
    "\n",
    "#         #get Validation performance\n",
    "#         bal_acc, sens, spec, auc, acc, prec, f1 = utils_exec_models.get_scores_from_predict(\n",
    "#             y_validation=y_valid, \n",
    "#             y_pred=y_pred, \n",
    "#         )\n",
    "\n",
    "#         # Store the Validation and Training performances\n",
    "#         performance_to_save = {\n",
    "#             'Scenario': scenario,\n",
    "#             'Model': model,\n",
    "#             'Estimator': estimator,\n",
    "#             # Validation performance\n",
    "#             'Valid_BalAcc': bal_acc,\n",
    "#             'Valid_Sens'  : sens,\n",
    "#             'Valid_Spec'  : spec,\n",
    "#             'Valid_f1'    : f1,\n",
    "#             'Valid_AUC'   : auc,\n",
    "#             'Valid_Acc'   : acc,\n",
    "#             'Valid_Prec'  : prec,\n",
    "#             #\n",
    "#             'Model_Hyperparams': model_params,\n",
    "#             'Estimator_Hyperparams': estimator_params,\n",
    "#         }\n",
    "#         #\n",
    "#         models_results.append(performance_to_save)\n",
    "\n",
    "        # create a dataFrame to store the performances and sort them\n",
    "        df_validation_performance = pd.DataFrame(models_results)\n",
    "\n",
    "        if df_validation_performance is None:\n",
    "            df_validation_performance = df_results_aux\n",
    "        else:\n",
    "            df_validation_performance = pd.concat([df_best_performances, df_results_aux])\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('Instance')\n",
    "        print(model_instance)\n",
    "        print('INFO')\n",
    "        print(model_info)\n",
    "        print('ERROR')\n",
    "        raise Exception(ex)\n",
    "       \n",
    "\n",
    "        \n",
    "# clear_output()\n",
    "       \n",
    "# create a dataFrame to store the performances and sort them\n",
    "df_validation_performance = pd.DataFrame(models_results)\n",
    "\n",
    "df_validation_performance = utils_exec_models.sort_performances_results(\n",
    "    df=df_validation_performance,\n",
    "    cols_order_to_sort=['Scenario', 'Model', 'Estimator', 'Valid_BalAcc', 'Valid_Sens', 'Valid_Spec'],\n",
    ")\n",
    "\n",
    "\n",
    "# save validation performance\n",
    "csv_file = os.path.abspath('exec_results/validation_results.csv')\n",
    "utils.save_to_csv(\n",
    "    df=df_validation_performance, \n",
    "    csv_file=csv_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# OTHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
